{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnCLGP6uEudq",
        "outputId": "7e2c9e30-4bcc-49fe-d540-8daed17779b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=6f75d66b35dc53c041471e4fc5f64009c6955736a672b1c30194adb1b0064254\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/25/f7/1cc0956978ae479e75140219088deb7a36f60459df242b1a72\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacytextblob\n",
            "  Downloading spacytextblob-4.0.0-py3-none-any.whl (4.5 kB)\n",
            "Requirement already satisfied: textblob<0.16.0,>=0.15.3 in /usr/local/lib/python3.8/dist-packages (from spacytextblob) (0.15.3)\n",
            "Requirement already satisfied: spacy<4.0,>=3.0 in /usr/local/lib/python3.8/dist-packages (from spacytextblob) (3.4.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (3.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (1.0.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (1.10.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (1.21.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (2.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (1.0.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (2.4.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (2.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (2.23.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (8.1.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (2.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (3.0.10)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (0.7.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (3.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (4.64.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (6.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (57.4.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0,>=3.0->spacytextblob) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<4.0,>=3.0->spacytextblob) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<4.0,>=3.0->spacytextblob) (4.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0,>=3.0->spacytextblob) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0,>=3.0->spacytextblob) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0,>=3.0->spacytextblob) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0,>=3.0->spacytextblob) (2022.12.7)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.8/dist-packages (from textblob<0.16.0,>=0.15.3->spacytextblob) (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob<0.16.0,>=0.15.3->spacytextblob) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob<0.16.0,>=0.15.3->spacytextblob) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk>=3.1->textblob<0.16.0,>=0.15.3->spacytextblob) (1.2.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4.0,>=3.0->spacytextblob) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4.0,>=3.0->spacytextblob) (0.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<4.0,>=3.0->spacytextblob) (2.0.1)\n",
            "Installing collected packages: spacytextblob\n",
            "Successfully installed spacytextblob-4.0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!pip install sklearn\n",
        "!pip install spacytextblob\n",
        "\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import spacy\n",
        "import scipy\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from spacytextblob.spacytextblob import SpacyTextBlob\n",
        "from sklearn.metrics import precision_score, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8npbkZYf1bj"
      },
      "outputs": [],
      "source": [
        "#Path of the file containing data for the project. Feel free to change depending on where the data is\n",
        "FILEPATH = 'subreddit_irony_data.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCrzyKvThnt6"
      },
      "outputs": [],
      "source": [
        "#Function to read data from csv file and load into a pandas dataframe\n",
        "def load_data():\n",
        "  df=pd.read_csv(FILEPATH)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfIUArPNRWjf"
      },
      "outputs": [],
      "source": [
        "#Function to get the comment text from Parent comments (and Parent's parent's comments) recursively.\n",
        "#Input Parameter:\n",
        "#id: Comment ID of a comment\n",
        "#df: The original complete dataframe with all the data\n",
        "#Returns a sentence with comments from all the parents(ancestors).\n",
        "\n",
        "def get_ancestors(id, df):\n",
        "  df1 = df[df['comment_id'] == id]\n",
        "  sen = ''\n",
        "  par = set()\n",
        "  for i in df1.index:\n",
        "    sen = sen + ' ' + df1['comment'][i]\n",
        "\n",
        "    # Store the parent IDs of the current comment in a set. This takes care of case where a given comment ID has more than 1 Parent IDs associated to it.\n",
        "    if not pd.isna(df1['parent_id'][i]):\n",
        "      par.add(df1['parent_id'][i])\n",
        "  \n",
        "  #Recursively calling the function for all of the Parent IDs related to this comment ID.\n",
        "  for pid in par:\n",
        "    sen = sen + ' ' + get_ancestors(pid, df)\n",
        "  return sen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSL8rJMUNOHf"
      },
      "outputs": [],
      "source": [
        "#Function to get all the comments appended with the comments from their parents\n",
        "#Input Parameter:\n",
        "#df: The original complete dataframe with all the data\n",
        "#Returns a list of strings with each item being the original comment appended to all the parents' comments.\n",
        "\n",
        "def nnp_sentence_prep(df):\n",
        "  nnp_sentences = []\n",
        "  for i in df.index:\n",
        "    #Check if the comment has a label. If it does then concat the comment text and thread title together\n",
        "    if not pd.isna(df['label'][i]):\n",
        "      sen = df['comment'][i] + ' ' + df['thread_title'][i]\n",
        "\n",
        "      #If the comment has a parent id then get the comments from all the ancestors and concat them to the current string\n",
        "      if not pd.isna(df['parent_id'][i]):\n",
        "        sen = sen + ' ' + get_ancestors(df['parent_id'][i], df)\n",
        "      nnp_sentences.append(sen)\n",
        "\n",
        "  return nnp_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osIZ-yXIEfmH"
      },
      "outputs": [],
      "source": [
        "#Function to get the sentiment(positive/negetive) of all the rows that have a label\n",
        "#Input Parameter:\n",
        "#raw_sentences: a list of strings representing the comments\n",
        "#Returns a list of strings with each item being the sentiment of the comment at that index.\n",
        "\n",
        "def sentiment_gathering(raw_sentences):\n",
        "  cache_path = \"parsed_sentiments.pickle\"\n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "  #Used spacytextblob to make sentiment prediction for the comments.\n",
        "  nlp.add_pipe('spacytextblob')\n",
        "  if os.path.exists(cache_path): \n",
        "    parsed_sentences = pickle.load(open(cache_path, 'rb'))\n",
        "  else:\n",
        "    parsed_sentences = []\n",
        "    for i,r in enumerate(raw_sentences):\n",
        "      parsed_sentences.append(nlp(r))\n",
        "    if cache_path is not None:\n",
        "      pickle.dump(parsed_sentences, open(cache_path, 'wb'))\n",
        "\n",
        "  sent = []\n",
        "  p = '+'\n",
        "  n = '-'\n",
        "  for tok in parsed_sentences:\n",
        "    pol = tok._.blob.polarity\n",
        "    pol = round(pol, 2)\n",
        "    #If polarity is greater than 0 then classify the sentiment of the sentence as positive else negetive\n",
        "    if pol > 0:\n",
        "      sent.append(p)\n",
        "    else:\n",
        "      sent.append(n)\n",
        "\n",
        "  return sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHGw5vMnMWHR"
      },
      "outputs": [],
      "source": [
        "#Function to preprocess the data for the NPP+ model\n",
        "#Input Parameter:\n",
        "#raw_sentences: a list of strings representing the comments concatenated with parents'(ancestors') comments\n",
        "#sent: a list of strings with each item being the sentiment of the comment at that index.\n",
        "#sub_redd: a list of strings with each item being the sub-reddit that the comment at that index belongs to.\n",
        "#Returns a list of tokenized strings(list of list of strings) with each token being (\"NNP+ sentiment sub-reddit\").\n",
        "\n",
        "def nnp_preprocessing(raw_sentences, sent, sub_redd):\n",
        "  cache_path = \"parsed_sentences.pickle\"\n",
        "  #Tokenizing comment sentences with spaCy\n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "  if os.path.exists(cache_path): \n",
        "    parsed_sentences = pickle.load(open(cache_path, 'rb'))\n",
        "  else:\n",
        "    parsed_sentences = []\n",
        "    for i,r in enumerate(raw_sentences):\n",
        "      parsed_sentences.append(nlp(r))\n",
        "    if cache_path is not None:\n",
        "       pickle.dump(parsed_sentences, open(cache_path, 'wb'))\n",
        "\n",
        "  preproc_sentences = []\n",
        "\n",
        "  for i, sentence in enumerate(parsed_sentences):\n",
        "    preproc_tokens = []\n",
        "    for token in sentence:\n",
        "      #Using only tokens tagged as NNP\n",
        "      if token.tag_ == 'NNP':\n",
        "        #Setting each token as (\"NNP+ sentiment sub-reddit\")\n",
        "        s = token.lemma_.lower() + ' ' + sent[i] + ' ' + str(sub_redd[i]).lower()\n",
        "        preproc_tokens.append(s)\n",
        "    \n",
        "    #Adding sentiment as an independent feature at the end of the list for each sentence.\n",
        "    preproc_tokens.append(sent[i])\n",
        "    preproc_sentences.append(preproc_tokens)\n",
        "\n",
        "  return preproc_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUxBwG8V3OGD"
      },
      "outputs": [],
      "source": [
        "#Function to preprocess the data for the Baseline model\n",
        "#Input Parameter:\n",
        "#raw_sentences: a list of strings representing the comment texts\n",
        "#Returns a list of list of strings after splitting each string on white space.\n",
        "\n",
        "def Baseline_preproc(raw_sentences):\n",
        "  preproc_X = [ str(x).split() for x in raw_sentences ]\n",
        "  return preproc_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47GbHKHLZtEc"
      },
      "outputs": [],
      "source": [
        "#Function to featurize the input for the model\n",
        "#Input Parameter:\n",
        "#preproc_X: Preprocessed Input for the model\n",
        "#dv: a DictVectorizer object. Passed only for the test data. Created during train data.\n",
        "#isTest: a boolean representing if the data is train data or test data\n",
        "#Returns a tuple of featurized input and the DictVectorizer object\n",
        "\n",
        "def featurize(preproc_X, dv=None, isTest = False):\n",
        "  dicts = []\n",
        "  for i in range(len(preproc_X)):\n",
        "    #Count the frequency of each uni-gram in each pre-processed sentence and save it as a dictionary.\n",
        "    cnt = Counter(preproc_X[i])\n",
        "    #Append the dictionary to the list of dictionaries\n",
        "    dicts.append(cnt)\n",
        "\n",
        "    if len(preproc_X[i])>1:\n",
        "      for j in range(len(preproc_X[i])-1):\n",
        "        s = (str(preproc_X[i][j]) + \"~\" + str(preproc_X[i][j+1]))\n",
        "        # For each bi-gram in a pre-processed sentence, increase its count in the dictionary if it is already preasent, otherwise initialize it to 1.\n",
        "        if s in dicts[i]:\n",
        "          dicts[i][s] += 1\n",
        "        else:\n",
        "          dicts[i][s] = 1\n",
        "  \n",
        "  dicts = np.array(dicts)\n",
        "  if isTest is False:\n",
        "    dv = DictVectorizer()\n",
        "    #Featurize the pre-processed input\n",
        "    X = dv.fit_transform(dicts)\n",
        "    return X, dv\n",
        "  else:\n",
        "    return dv.transform(dicts), dv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zABP57Dri4aE"
      },
      "outputs": [],
      "source": [
        "#Function to create a Stocgastic Gradient Descent model.\n",
        "#Input Parameter:\n",
        "#alpha: The constant that multiplies the regularization term.\n",
        "#Returns the model used for prediction\n",
        "\n",
        "def create_classifier(alpha):\n",
        "  #The loss used here is log-loss and the class weight has been forced to be balanced.\n",
        "  clf = SGDClassifier(loss='log', class_weight='balanced', alpha=alpha)\n",
        "  return clf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e597YV5ckkkN"
      },
      "outputs": [],
      "source": [
        "#Function to evaluate the model.\n",
        "#Input Parameter:\n",
        "#X: Test Data\n",
        "#y_true: Labels\n",
        "#model: the model to be evaluated\n",
        "#Returns a tuple of precision and recall of the model\n",
        "\n",
        "def evaluate_model(X, y_true, model):\n",
        "  #Get predictions from the model so that it can be used to get other metrics\n",
        "  y_pred = model.predict(X)\n",
        "  #Precision Score\n",
        "  prec = precision_score(y_true, y_pred)\n",
        "  #Recall Score\n",
        "  rec = recall_score(y_true, y_pred)\n",
        "  return prec, rec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "funO3jqJSpoE"
      },
      "outputs": [],
      "source": [
        "#Function to run repeated KFold cross validation on the data.\n",
        "#Input Parameter:\n",
        "#X: Pre-processed input data\n",
        "#y: Labels\n",
        "#k: Number of folds the data is supposed to be split into\n",
        "#reps: Number of repetitions of KFold\n",
        "#alpha: alpha parameter for the model\n",
        "#Returns a tuple of a list of precision scores and a list of recall scores of the model\n",
        "\n",
        "def run_rkfold_crossval(X, y, k=5, reps=2, alpha=1e-2):\n",
        "  prec_score = []\n",
        "  rec_score = []\n",
        "  #Initialise RepeatedKFold object\n",
        "  rkf = RepeatedKFold(n_splits=k, n_repeats=reps, random_state=2652124)\n",
        "  y1 = np.array(y, dtype=object)\n",
        "  #Create untrained model with chosen alpha\n",
        "  clf_u = create_classifier(alpha)\n",
        "\n",
        "  #This loops through the n_repeats(reps) iterations of KFold \n",
        "  #Each iteration of KFold loops loops through the entire dataset n_splits(k) times\n",
        "  #Therefore, this loops through the entire dataset (n_repeats * n_splits) times = (reps * k) times\n",
        "  for train_index , test_index in rkf.split(X):\n",
        "    X = np.array(X, dtype=object)\n",
        "    \n",
        "    #Splitting the data in train and test.\n",
        "    #k-1 portions are used for train and 1 for test\n",
        "    #Since k is 5, the data is split as 80% train and 20% test\n",
        "    X_train1 , X_test1 = X[train_index], X[test_index]\n",
        "    y_train , y_test = y1[train_index] , y1[test_index]\n",
        "    \n",
        "    #Getting training and testing features from the data\n",
        "    X_train_feat_spr, dv = featurize(X_train1)\n",
        "    X_train_feat = scipy.sparse.csr_matrix.toarray(X_train_feat_spr)\n",
        "    X_test_feat_spr, _ = featurize(X_test1, dv, True)\n",
        "    X_test_feat = scipy.sparse.csr_matrix.toarray(X_test_feat_spr)\n",
        "\n",
        "    #Train the model using training features and labels\n",
        "    clf = clf_u.fit(X_train_feat, list(y_train))\n",
        "\n",
        "    #Evaluate the model using test features and labels to get precision and recall\n",
        "    prec, rec = evaluate_model(X_test_feat, list(y_test), clf)\n",
        "\n",
        "    #Keep track of all the precision and recall\n",
        "    prec_score.append(prec)\n",
        "    rec_score.append(rec)\n",
        "\n",
        "  return prec_score, rec_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iczaO2lhNKu"
      },
      "outputs": [],
      "source": [
        "#Function to calculate statistics for a given list of metric scores.\n",
        "#Input Parameter:\n",
        "#score: list of metric scores\n",
        "#Returns a tuple of mean, median, 25th percentile, 75th percentile values of the list of metric scores\n",
        "\n",
        "def calc_stats(score):\n",
        "  mean_val = np.mean(score)\n",
        "  median_val = np.median(score)\n",
        "  val_25 = np.percentile(score, 25)\n",
        "  val_75 = np.percentile(score, 75)\n",
        "\n",
        "  return mean_val, median_val, val_25, val_75"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58_BpQzNqUWT",
        "outputId": "ee0287e5-c102-49a7-8b8f-9ba34d99bf4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bag of Words Baseline\n",
            "Precision\n",
            "0.09 0.09 0.06 0.13\n",
            "Recall\n",
            "0.14 0.13 0.08 0.17\n",
            "NP Sentiment Context Model\n",
            "Precision\n",
            "0.15 0.15 0.11 0.18\n",
            "Recall\n",
            "0.31 0.31 0.26 0.37\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "  #Number of folds the data is supposed to be split into\n",
        "  K_FOLD = 5\n",
        "  #Number of repetitions of KFold. The total number of iterations through the entire dataset is going to be (bow_reps * K_FOLD) = (4*5) = 20 ITERATIONS through the entire dataset\n",
        "  bow_reps = 4\n",
        "  #alpha parameter for the BOW model\n",
        "  bow_alpha = 1e-2\n",
        "\n",
        "  #Load the input data into pandas dataframe\n",
        "  df1 = load_data()\n",
        "\n",
        "  #Create another dataframe that doesn't have rows with no labels\n",
        "  df = df1[df1['label'].notna()]\n",
        "\n",
        "  #Get comments that have some label\n",
        "  raw_sentences = df['comment']\n",
        "  #Get labels associated with the comments\n",
        "  labels = df['label']\n",
        "\n",
        "  #Preprocess data for the Baseline BOW model\n",
        "  preproc_baseline_sent = Baseline_preproc(raw_sentences)\n",
        "  #Run RepeatedKFold on the the data, train the Baseline BOW model on the data and evaluate the model to get the precision and recall scores\n",
        "  bow_baseline_model_prec_score, bow_baseline_model_rec_score = run_rkfold_crossval(preproc_baseline_sent, labels, k=K_FOLD, reps=bow_reps, alpha=bow_alpha)\n",
        "  #Calculate the required stats on precision and recall for the BOW model\n",
        "  bow_mean_precision, bow_median_precision, bow_25th_perc_precision, bow_75th_perc_precision = calc_stats(bow_baseline_model_prec_score)\n",
        "  bow_mean_recall, bow_median_recall, bow_25th_perc_recall, bow_75th_perc_recall = calc_stats(bow_baseline_model_rec_score)\n",
        "\n",
        "  #Number of repetitions of KFold. The total number of iterations through the entire dataset is going to be (nnp_reps * K_FOLD) = (10*5) = 50 ITERATIONS through the entire dataset\n",
        "  nnp_reps = 10\n",
        "  #alpha parameter for the NNP model\n",
        "  nnp_alpha = 1e-1\n",
        "  #Get sentiments for the sentences that have a label associated with them\n",
        "  sent = list(sentiment_gathering(raw_sentences))\n",
        "  #Use the entire dataframe (without removing the rows with NA labels) to get the comment texts for each comment sentence appended to the comment texts of its parents.\n",
        "  nnp_raw_sentences = nnp_sentence_prep(df1)\n",
        "  #List of subreddit which the comment belongs to\n",
        "  sub_redd = list(df['subreddit'])\n",
        "  #Preprocess data for the Baseline NNP model\n",
        "  preproc_nnp_sent = nnp_preprocessing(nnp_raw_sentences, sent, sub_redd)\n",
        "  #Run RepeatedKFold on the the data, train the NNP model on the data and evaluate the model to get the precision and recall scores\n",
        "  np_prec_score, np_rec_score = run_rkfold_crossval(preproc_nnp_sent, labels, k=K_FOLD, reps=nnp_reps, alpha=nnp_alpha)\n",
        "  #Calculate the required stats on precision and recall for the NNP model\n",
        "  np_mean_precision, np_median_precision, np_25th_perc_precision, np_75th_perc_precision = calc_stats(np_prec_score)\n",
        "  np_mean_recall, np_median_recall, np_25th_perc_recall, np_75th_perc_recall = calc_stats(np_rec_score)\n",
        "  \n",
        "  #Printing results\n",
        "\n",
        "  def fformat(f):\n",
        "    return \"%.2f\" % f\n",
        "\n",
        "  print(\"Bag of Words Baseline\")\n",
        "  print(\"Precision\")\n",
        "  print(fformat(bow_mean_precision), fformat(bow_median_precision), fformat(bow_25th_perc_precision), fformat(bow_75th_perc_precision))\n",
        "  print(\"Recall\")\n",
        "  print(fformat(bow_mean_recall), fformat(bow_median_recall), fformat(bow_25th_perc_recall), fformat(bow_75th_perc_recall))\n",
        "\n",
        "  print(\"NP Sentiment Context Model\")\n",
        "  print(\"Precision\")\n",
        "  print(fformat(np_mean_precision), fformat(np_median_precision), fformat(np_25th_perc_precision), fformat(np_75th_perc_precision))\n",
        "  print(\"Recall\")\n",
        "  print(fformat(np_mean_recall), fformat(np_median_recall), fformat(np_25th_perc_recall), fformat(np_75th_perc_recall))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0i_9NPBeM_I"
      },
      "source": [
        "#<u>Output Metrics of the 2 Models:</u>\n",
        "\n",
        "##<u>Bag of Words Baseline:</u>\n",
        "###Precision\n",
        "Mean: 0.09 | Median: 0.09 | 25th Percentile: 0.06 | 75th Percentile: 0.13\n",
        "###Recall\n",
        "Mean: 0.14 | Median: 0.13 | 25th Percentile: 0.08 | 75th Percentile: 0.17\n",
        "\n",
        "##<u>NP Sentiment Context Model:</u>\n",
        "###Precision\n",
        "Mean: 0.15 | Median: 0.15 | 25th Percentile: 0.11 | 75th Percentile: 0.18\n",
        "###Recall\n",
        "Mean: 0.31 | Median: 0.31 | 25th Percentile: 0.26 | 75th Percentile: 0.37"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
